{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sciform/sciform-hwz-ai-in-controlling/blob/main/lecture_2/2_4_ts_04_forecasting_with_ml_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHG6wUbvifuX"
      },
      "source": [
        "# Forecasting with machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vidayERjaO5q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqWabzlJ63nL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Print Keras version\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg1hfKCPldZG"
      },
      "outputs": [],
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n",
        "    plt.plot(time[start:end], series[start:end], format, label=label)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "\n",
        "def white_noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "iL2DDjV3lel6",
        "outputId": "b2b71f9f-b0de-42e9-a84a-517a4faae001"
      },
      "outputs": [],
      "source": [
        "time = np.arange(4 * 365 + 1)\n",
        "\n",
        "slope = 0.05\n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "\n",
        "noise_level = 5\n",
        "noise = white_noise(time, noise_level, seed=42)\n",
        "\n",
        "series += noise\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, series)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViWVB9qd8OIR"
      },
      "source": [
        "## Forecasting with a \"simple\" Deep Learning Method\n",
        "\n",
        "We choose the simplest Deep Learning Model possible, which consists\n",
        "of 1 layer and 1 unit. This model is equivalent to linear regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C01ESmJBqlI"
      },
      "source": [
        "First, we will train a model to forecast the next step given the previous 30 steps, therefore, we need to create a dataset of 30-step windows for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tl-0BOKkEtk"
      },
      "outputs": [],
      "source": [
        "def window_dataset(series, window_size, batch_size=32,\n",
        "                   shuffle_buffer=1000):\n",
        "    \"\"\"\n",
        "    Generate a data set in the format\n",
        "    [X in window_size, y]\n",
        "    [0,1,2,...., window_size-1, window_size]\n",
        "    [1,2,3,...., window_size, window_size+1]\n",
        "    ....\n",
        "    \"\"\"\n",
        "    # convert ndarray into tensorflow data set format\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    # choose window size for X and 1 for y\n",
        "    # shift by 1 to generate the next one and so on\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    # shuffle all dataset entries to break correlations\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    # restore window : all X entries and then y\n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IxD9CMWByqN"
      },
      "source": [
        "We split the data into a training and validation data set. (We omit the test set, which would be also necessary to run an independent test.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmp1JXKxk9Vb"
      },
      "outputs": [],
      "source": [
        "# do a sequential split here\n",
        "# first - more historic - part of the time series is the training data\n",
        "# second - more current - part of the time series is the validation data\n",
        "# No random shuffling here, as we have to preserve the sequential order\n",
        "\n",
        "split_time = 1000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IvwAFn8OIc"
      },
      "source": [
        "### Linear Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvUnt_xwE6mY"
      },
      "source": [
        "#### Linear model with an estimated learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieOKdcEQ0A6k",
        "outputId": "d7fcc2c2-8312-45be-9baa-905127637eab"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# choose windows size\n",
        "window_size = 30\n",
        "\n",
        "# create training data set X_i = {0,1,2,..., window_size-1} of length window_size, y = value of next time step\n",
        "train_set = window_dataset(x_train, window_size)\n",
        "# create validation data set\n",
        "valid_set = window_dataset(x_valid, window_size)\n",
        "\n",
        "# create \"Deep Learning Model\" with 1 layer and 1 unit (this is equivalent to a function with 1 parameter w = Linear Regression)\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Input(shape=[window_size]),\n",
        "  keras.layers.Dense(1), # , input_shape=[window_size])\n",
        "])\n",
        "\n",
        "# choose Stochastic Gradient Descent (SDG) to solve the resulting optimization problem\n",
        "# lr : learning rate\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9)\n",
        "\n",
        "# collect the remaining information\n",
        "# optimization or loss function = Huber loss\n",
        "# metrics = mean absolute error (mae)\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# finally - train the model with 100 epochs and the validation set\n",
        "model.fit(train_set, epochs=100, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YakYDXkDM-b"
      },
      "source": [
        "#### Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9VcYrAKCf6p"
      },
      "source": [
        "The accuracy of the model is highly dependent on the learning rate. Now we use the Learning Rate Scheduler first to obtain an optimal learning rate and see whether we can improve the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3N8AGRM8OIc",
        "outputId": "b023bee8-8311-490d-9f95-44225efd22c5"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "\n",
        "train_set = window_dataset(x_train, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Input(shape=[window_size]),\n",
        "  keras.layers.Dense(1) #, input_shape=[window_size])\n",
        "])\n",
        "\n",
        "# learning rate scheduler runs over a range of learning rates\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 30))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9)\n",
        "\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "\n",
        "# add callback with the learning rate scheduler\n",
        "lr_opt_model = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPwNnGGpEM7f"
      },
      "source": [
        "We can plot the loss now for every learning rate and pick the learning rate, where the loss appears to minimal but still stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "PF9e7IDm8OId",
        "outputId": "15dbb319-3f18-4f9f-bdef-0d9525bdf6c9"
      },
      "outputs": [],
      "source": [
        "plt.semilogx(lr_opt_model.history[\"learning_rate\"], lr_opt_model.history[\"loss\"])\n",
        "plt.axis([1e-6, 1e-3, 0, 20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yov3b1ADULB"
      },
      "source": [
        "#### Linear model with optimized learning rate\n",
        "\n",
        "Now we use the learning rate, which we determined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMNwyIFE8OIf",
        "outputId": "c5d3fb0c-ae86-4f44-f9d5-828a4530f45f"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "train_set = window_dataset(x_train, window_size)\n",
        "valid_set = window_dataset(x_valid, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "   keras.layers.Input(shape=[window_size]),\n",
        "  keras.layers.Dense(1), # input_shape=[window_size])\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9)\n",
        "\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10)\n",
        "model.fit(train_set, epochs=500,\n",
        "          validation_data=valid_set,\n",
        "          callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bei-_PmsFy1K"
      },
      "source": [
        "#### Forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eaAX9g_jS5W"
      },
      "outputs": [],
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnIWROQ08OIj",
        "outputId": "208ac649-13a7-43da-d936-3a9941d46784"
      },
      "outputs": [],
      "source": [
        "# forecast based on linear deep learning model\n",
        "lin_forecast = model_forecast(model, series[split_time - window_size:-1], window_size)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "F-nftslfgQJs",
        "outputId": "f6abf178-bafd-4731-c5ef-380c8a2e9d3a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, lin_forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4E_jXktf7iv",
        "outputId": "a66663da-2e79-40cf-c2ad-8d0bfe00a8e5"
      },
      "outputs": [],
      "source": [
        "keras.metrics.mean_absolute_error(x_valid, lin_forecast).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nEM33dZ8OIp"
      },
      "source": [
        "### Dense Model Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhGTv4G_8OIp"
      },
      "outputs": [],
      "source": [
        "# use the following model first - try to improve\n",
        "\n",
        "\n",
        "# this model consists of 3 layer with 10 or 1 units and a nonlinear activation function called \"relu\"\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Input(shape=[window_size]),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swoZh89nDavy",
        "outputId": "a1347d6b-90d1-4be7-b284-85a48d501641"
      },
      "outputs": [],
      "source": [
        "# get inspired from the linear model\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "\n",
        "train_set = window_dataset(x_train, window_size)\n",
        "\n",
        "\n",
        "# learning rate scheduler runs over a range of learning rates\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 30))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9)\n",
        "\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "\n",
        "# add callback with the learning rate scheduler\n",
        "lr_opt_model = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "A94aG9Z18z-d",
        "outputId": "e4f73399-a29e-440c-8ff7-da179f6d6623"
      },
      "outputs": [],
      "source": [
        "plt.semilogx(lr_opt_model.history[\"learning_rate\"], lr_opt_model.history[\"loss\"])\n",
        "plt.axis([1e-6, 1e-3, 0, 20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdBBo34a8pcW",
        "outputId": "6788b967-78ed-4504-ccd3-990f415b26f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 18.2996 - mae: 18.7937 - learning_rate: 19.9526\n",
            "Epoch 221/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 19.9770 - mae: 20.4725 - learning_rate: 21.5443\n",
            "Epoch 222/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 17.4568 - mae: 17.9536 - learning_rate: 23.2631\n",
            "Epoch 223/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 21.2951 - mae: 21.7893 - learning_rate: 25.1189\n",
            "Epoch 224/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.8894 - mae: 18.3840 - learning_rate: 27.1227\n",
            "Epoch 225/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.8015 - mae: 20.2931 - learning_rate: 29.2864\n",
            "Epoch 226/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 18.1619 - mae: 18.6590 - learning_rate: 31.6228\n",
            "Epoch 227/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6506s\u001b[0m 4ms/step - loss: 21.7594 - mae: 22.2514 - learning_rate: 34.1455\n",
            "Epoch 228/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 22.9014 - mae: 23.3969 - learning_rate: 36.8694\n",
            "Epoch 229/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 18.4339 - mae: 18.9295 - learning_rate: 39.8107\n",
            "Epoch 230/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 34.1532 - mae: 34.6524 - learning_rate: 42.9866\n",
            "Epoch 231/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.1320 - mae: 19.6263 - learning_rate: 46.4159\n",
            "Epoch 232/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.4409 - mae: 31.9390 - learning_rate: 50.1187\n",
            "Epoch 233/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.9848 - mae: 21.4789 - learning_rate: 54.1170\n",
            "Epoch 234/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.1784 - mae: 20.6744 - learning_rate: 58.4341\n",
            "Epoch 235/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.7464 - mae: 25.2424 - learning_rate: 63.0957\n",
            "Epoch 236/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33.7008 - mae: 34.1985 - learning_rate: 68.1292\n",
            "Epoch 237/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 28.8640 - mae: 29.3600 - learning_rate: 73.5642\n",
            "Epoch 238/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36.2595 - mae: 36.7568 - learning_rate: 79.4328\n",
            "Epoch 239/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.9381 - mae: 29.4367 - learning_rate: 85.7696\n",
            "Epoch 240/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.7120 - mae: 34.2091 - learning_rate: 92.6119\n",
            "Epoch 241/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.3128 - mae: 41.8113 - learning_rate: 100.0000\n",
            "Epoch 242/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84.9245 - mae: 85.4238 - learning_rate: 107.9775\n",
            "Epoch 243/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.1626 - mae: 112.6625 - learning_rate: 116.5914\n",
            "Epoch 244/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.6907 - mae: 115.1907 - learning_rate: 125.8925\n",
            "Epoch 245/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.8283 - mae: 135.3282 - learning_rate: 135.9356\n",
            "Epoch 246/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 148.0744 - mae: 148.5744 - learning_rate: 146.7799\n",
            "Epoch 247/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 146.2749 - mae: 146.7749 - learning_rate: 158.4893\n",
            "Epoch 248/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91.1940 - mae: 91.6940 - learning_rate: 171.1328\n",
            "Epoch 249/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.0294 - mae: 97.5294 - learning_rate: 184.7850\n",
            "Epoch 250/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.3022 - mae: 102.8022 - learning_rate: 199.5262\n",
            "Epoch 251/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.9003 - mae: 110.3999 - learning_rate: 215.4435\n",
            "Epoch 252/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.2243 - mae: 131.7243 - learning_rate: 232.6305\n",
            "Epoch 253/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 160.2351 - mae: 160.7342 - learning_rate: 251.1886\n",
            "Epoch 254/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 198.4332 - mae: 198.9326 - learning_rate: 271.2273\n",
            "Epoch 255/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.1644 - mae: 147.6639 - learning_rate: 292.8645\n",
            "Epoch 256/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 240.2845 - mae: 240.7842 - learning_rate: 316.2278\n",
            "Epoch 257/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 316.5186 - mae: 317.0186 - learning_rate: 341.4549\n",
            "Epoch 258/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 234.9436 - mae: 235.4436 - learning_rate: 368.6945\n",
            "Epoch 259/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 338.4219 - mae: 338.9219 - learning_rate: 398.1072\n",
            "Epoch 260/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 487.0027 - mae: 487.5027 - learning_rate: 429.8662\n",
            "Epoch 261/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 411.0643 - mae: 411.5642 - learning_rate: 464.1589\n",
            "Epoch 262/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 444.7580 - mae: 445.2575 - learning_rate: 501.1872\n",
            "Epoch 263/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 522.0768 - mae: 522.5767 - learning_rate: 541.1696\n",
            "Epoch 264/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 439.9606 - mae: 440.4602 - learning_rate: 584.3414\n",
            "Epoch 265/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 171.3126 - mae: 171.8126 - learning_rate: 630.9573\n",
            "Epoch 266/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 187.4603 - mae: 187.9603 - learning_rate: 681.2921\n",
            "Epoch 267/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 201.5582 - mae: 202.0582 - learning_rate: 735.6423\n",
            "Epoch 268/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 218.1177 - mae: 218.6177 - learning_rate: 794.3282\n",
            "Epoch 269/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 236.2872 - mae: 236.7872 - learning_rate: 857.6959\n",
            "Epoch 270/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 253.7257 - mae: 254.2257 - learning_rate: 926.1187\n",
            "Epoch 271/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 275.0042 - mae: 275.5042 - learning_rate: 1000.0000\n",
            "Epoch 272/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 297.3110 - mae: 297.8110 - learning_rate: 1079.7751\n",
            "Epoch 273/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 321.2897 - mae: 321.7897 - learning_rate: 1165.9144\n",
            "Epoch 274/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 344.1706 - mae: 344.6706 - learning_rate: 1258.9254\n",
            "Epoch 275/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373.9426 - mae: 374.4426 - learning_rate: 1359.3564\n",
            "Epoch 276/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 403.9426 - mae: 404.4426 - learning_rate: 1467.7993\n",
            "Epoch 277/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 435.5399 - mae: 436.0399 - learning_rate: 1584.8932\n",
            "Epoch 278/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 471.2238 - mae: 471.7238 - learning_rate: 1711.3282\n",
            "Epoch 279/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 508.0434 - mae: 508.5434 - learning_rate: 1847.8499\n",
            "Epoch 280/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 549.4210 - mae: 549.9210 - learning_rate: 1995.2623\n",
            "Epoch 281/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 592.3251 - mae: 592.8251 - learning_rate: 2154.4346\n",
            "Epoch 282/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 638.5769 - mae: 639.0769 - learning_rate: 2326.3052\n",
            "Epoch 283/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 689.8208 - mae: 690.3208 - learning_rate: 2511.8865\n",
            "Epoch 284/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 745.9215 - mae: 746.4215 - learning_rate: 2712.2725\n",
            "Epoch 285/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 807.5040 - mae: 808.0040 - learning_rate: 2928.6445\n",
            "Epoch 286/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 870.5878 - mae: 871.0878 - learning_rate: 3162.2776\n",
            "Epoch 287/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 939.6355 - mae: 940.1355 - learning_rate: 3414.5488\n",
            "Epoch 288/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1013.9196 - mae: 1014.4196 - learning_rate: 3686.9451\n",
            "Epoch 289/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1094.7815 - mae: 1095.2815 - learning_rate: 3981.0718\n",
            "Epoch 290/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1181.3389 - mae: 1181.8389 - learning_rate: 4298.6621\n",
            "Epoch 291/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1276.0881 - mae: 1276.5881 - learning_rate: 4641.5889\n",
            "Epoch 292/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1378.4229 - mae: 1378.9229 - learning_rate: 5011.8726\n",
            "Epoch 293/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1488.7876 - mae: 1489.2876 - learning_rate: 5411.6953\n",
            "Epoch 294/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1605.1000 - mae: 1605.6000 - learning_rate: 5843.4141\n",
            "Epoch 295/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1735.3367 - mae: 1735.8367 - learning_rate: 6309.5732\n",
            "Epoch 296/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1874.1914 - mae: 1874.6914 - learning_rate: 6812.9209\n",
            "Epoch 297/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2023.9714 - mae: 2024.4714 - learning_rate: 7356.4224\n",
            "Epoch 298/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2184.4634 - mae: 2184.9634 - learning_rate: 7943.2822\n",
            "Epoch 299/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2359.0063 - mae: 2359.5063 - learning_rate: 8576.9590\n",
            "Epoch 300/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2547.7634 - mae: 2548.2634 - learning_rate: 9261.1875\n",
            "Epoch 301/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2752.1726 - mae: 2752.6726 - learning_rate: 10000.0000\n",
            "Epoch 302/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2970.9038 - mae: 2971.4038 - learning_rate: 10797.7520\n",
            "Epoch 303/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3207.6694 - mae: 3208.1694 - learning_rate: 11659.1436\n",
            "Epoch 304/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3464.6799 - mae: 3465.1799 - learning_rate: 12589.2539\n",
            "Epoch 305/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3739.8147 - mae: 3740.3147 - learning_rate: 13593.5635\n",
            "Epoch 306/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4037.3435 - mae: 4037.8435 - learning_rate: 14677.9932\n",
            "Epoch 307/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4359.5806 - mae: 4360.0806 - learning_rate: 15848.9316\n",
            "Epoch 308/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4708.2827 - mae: 4708.7827 - learning_rate: 17113.2832\n",
            "Epoch 309/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5083.0767 - mae: 5083.5767 - learning_rate: 18478.4980\n",
            "Epoch 310/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5490.1318 - mae: 5490.6318 - learning_rate: 19952.6230\n",
            "Epoch 311/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5928.1172 - mae: 5928.6172 - learning_rate: 21544.3477\n",
            "Epoch 312/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6400.7026 - mae: 6401.2026 - learning_rate: 23263.0508\n",
            "Epoch 313/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6911.0703 - mae: 6911.5703 - learning_rate: 25118.8652\n",
            "Epoch 314/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7462.5327 - mae: 7463.0327 - learning_rate: 27122.7266\n",
            "Epoch 315/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8057.2686 - mae: 8057.7686 - learning_rate: 29286.4453\n",
            "Epoch 316/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8702.7246 - mae: 8703.2246 - learning_rate: 31622.7773\n",
            "Epoch 317/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9393.6572 - mae: 9394.1572 - learning_rate: 34145.4883\n",
            "Epoch 318/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10144.3945 - mae: 10144.8945 - learning_rate: 36869.4492\n",
            "Epoch 319/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10954.1797 - mae: 10954.6797 - learning_rate: 39810.7188\n",
            "Epoch 320/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11830.3232 - mae: 11830.8232 - learning_rate: 42986.6250\n",
            "Epoch 321/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 12771.5000 - mae: 12772.0000 - learning_rate: 46415.8867\n",
            "Epoch 322/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13791.7969 - mae: 13792.2969 - learning_rate: 50118.7227\n",
            "Epoch 323/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14890.5566 - mae: 14891.0566 - learning_rate: 54116.9531\n",
            "Epoch 324/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16077.2676 - mae: 16077.7676 - learning_rate: 58434.1406\n",
            "Epoch 325/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17360.4355 - mae: 17360.9355 - learning_rate: 63095.7344\n",
            "Epoch 326/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18746.5645 - mae: 18747.0645 - learning_rate: 68129.2031\n",
            "Epoch 327/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20242.3730 - mae: 20242.8730 - learning_rate: 73564.2266\n",
            "Epoch 328/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21856.7637 - mae: 21857.2637 - learning_rate: 79432.8203\n",
            "Epoch 329/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23601.0605 - mae: 23601.5605 - learning_rate: 85769.5938\n",
            "Epoch 330/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25483.0820 - mae: 25483.5820 - learning_rate: 92611.8750\n",
            "Epoch 331/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27515.9043 - mae: 27516.4043 - learning_rate: 100000.0000\n",
            "Epoch 332/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29712.0137 - mae: 29712.5137 - learning_rate: 107977.5156\n",
            "Epoch 333/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32083.8340 - mae: 32084.3340 - learning_rate: 116591.4375\n",
            "Epoch 334/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34641.5703 - mae: 34642.0703 - learning_rate: 125892.5391\n",
            "Epoch 335/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37404.5820 - mae: 37405.0820 - learning_rate: 135935.6406\n",
            "Epoch 336/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40388.1289 - mae: 40388.6289 - learning_rate: 146779.9219\n",
            "Epoch 337/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43610.1289 - mae: 43610.6289 - learning_rate: 158489.3125\n",
            "Epoch 338/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 47090.5703 - mae: 47091.0703 - learning_rate: 171132.8281\n",
            "Epoch 339/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50847.5586 - mae: 50848.0586 - learning_rate: 184784.9844\n",
            "Epoch 340/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54903.7969 - mae: 54904.2969 - learning_rate: 199526.2344\n",
            "Epoch 341/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59284.2461 - mae: 59284.7461 - learning_rate: 215443.4688\n",
            "Epoch 342/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 64012.4727 - mae: 64012.9727 - learning_rate: 232630.5000\n",
            "Epoch 343/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69119.6328 - mae: 69120.1328 - learning_rate: 251188.6406\n",
            "Epoch 344/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74632.4141 - mae: 74632.9141 - learning_rate: 271227.2500\n",
            "Epoch 345/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80588.3281 - mae: 80588.8281 - learning_rate: 292864.4688\n",
            "Epoch 346/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 87016.3906 - mae: 87016.8906 - learning_rate: 316227.7812\n",
            "Epoch 347/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93958.3672 - mae: 93958.8672 - learning_rate: 341454.8750\n",
            "Epoch 348/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101453.3594 - mae: 101453.8594 - learning_rate: 368694.5000\n",
            "Epoch 349/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109546.5781 - mae: 109547.0781 - learning_rate: 398107.1562\n",
            "Epoch 350/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118285.9531 - mae: 118286.4531 - learning_rate: 429866.2500\n",
            "Epoch 351/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127722.3516 - mae: 127722.8516 - learning_rate: 464158.8750\n",
            "Epoch 352/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 137912.0312 - mae: 137912.5312 - learning_rate: 501187.2188\n",
            "Epoch 353/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148914.4688 - mae: 148914.9688 - learning_rate: 541169.5000\n",
            "Epoch 354/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 160792.1094 - mae: 160792.6094 - learning_rate: 584341.4375\n",
            "Epoch 355/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173621.6250 - mae: 173622.1250 - learning_rate: 630957.3750\n",
            "Epoch 356/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 187470.5312 - mae: 187471.0312 - learning_rate: 681292.0625\n",
            "Epoch 357/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 202426.7031 - mae: 202427.2031 - learning_rate: 735642.2500\n",
            "Epoch 358/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 218575.4688 - mae: 218575.9688 - learning_rate: 794328.2500\n",
            "Epoch 359/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 236011.5156 - mae: 236012.0156 - learning_rate: 857695.8750\n",
            "Epoch 360/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 254838.4375 - mae: 254838.9375 - learning_rate: 926118.7500\n",
            "Epoch 361/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275170.1250 - mae: 275170.6250 - learning_rate: 1000000.0000\n",
            "Epoch 362/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 297119.6562 - mae: 297120.1562 - learning_rate: 1079775.1250\n",
            "Epoch 363/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 320824.0625 - mae: 320824.5625 - learning_rate: 1165914.3750\n",
            "Epoch 364/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 346418.5938 - mae: 346419.0938 - learning_rate: 1258925.3750\n",
            "Epoch 365/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 374053.1562 - mae: 374053.6875 - learning_rate: 1359356.3750\n",
            "Epoch 366/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 403893.7188 - mae: 403894.2500 - learning_rate: 1467799.2500\n",
            "Epoch 367/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 436115.7812 - mae: 436116.2500 - learning_rate: 1584893.2500\n",
            "Epoch 368/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 470905.7812 - mae: 470906.2500 - learning_rate: 1711328.2500\n",
            "Epoch 369/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 508472.5312 - mae: 508473.0000 - learning_rate: 1847849.7500\n",
            "Epoch 370/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 549036.9375 - mae: 549037.5000 - learning_rate: 1995262.3750\n",
            "Epoch 371/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 592835.0000 - mae: 592835.5625 - learning_rate: 2154434.7500\n",
            "Epoch 372/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 640128.4375 - mae: 640128.9375 - learning_rate: 2326305.0000\n",
            "Epoch 373/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 691194.1875 - mae: 691194.6875 - learning_rate: 2511886.5000\n",
            "Epoch 374/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 746337.5000 - mae: 746338.0625 - learning_rate: 2712272.5000\n",
            "Epoch 375/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 805876.5625 - mae: 805877.0625 - learning_rate: 2928644.5000\n",
            "Epoch 376/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 870163.3125 - mae: 870163.8125 - learning_rate: 3162277.7500\n",
            "Epoch 377/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 939581.5625 - mae: 939582.0625 - learning_rate: 3414548.7500\n",
            "Epoch 378/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1014534.8750 - mae: 1014535.3750 - learning_rate: 3686945.0000\n",
            "Epoch 379/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1095472.7500 - mae: 1095473.3750 - learning_rate: 3981071.7500\n",
            "Epoch 380/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1182862.0000 - mae: 1182862.6250 - learning_rate: 4298662.5000\n",
            "Epoch 381/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1277227.5000 - mae: 1277227.8750 - learning_rate: 4641589.0000\n",
            "Epoch 382/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1379116.5000 - mae: 1379117.0000 - learning_rate: 5011872.5000\n",
            "Epoch 383/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1489136.2500 - mae: 1489136.7500 - learning_rate: 5411695.5000\n",
            "Epoch 384/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1607934.2500 - mae: 1607934.7500 - learning_rate: 5843414.0000\n",
            "Epoch 385/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1736203.0000 - mae: 1736203.5000 - learning_rate: 6309573.5000\n",
            "Epoch 386/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1874713.1250 - mae: 1874713.6250 - learning_rate: 6812920.5000\n",
            "Epoch 387/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2024266.2500 - mae: 2024267.0000 - learning_rate: 7356422.5000\n",
            "Epoch 388/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2185755.0000 - mae: 2185755.7500 - learning_rate: 7943282.5000\n",
            "Epoch 389/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2360119.5000 - mae: 2360120.0000 - learning_rate: 8576959.0000\n",
            "Epoch 390/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2548404.0000 - mae: 2548404.5000 - learning_rate: 9261187.0000\n",
            "Epoch 391/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2751700.0000 - mae: 2751700.5000 - learning_rate: 10000000.0000\n",
            "Epoch 392/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2971222.0000 - mae: 2971222.2500 - learning_rate: 10797752.0000\n",
            "Epoch 393/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3208244.7500 - mae: 3208245.0000 - learning_rate: 11659144.0000\n",
            "Epoch 394/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3464188.7500 - mae: 3464189.2500 - learning_rate: 12589254.0000\n",
            "Epoch 395/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3740539.5000 - mae: 3740540.0000 - learning_rate: 13593564.0000\n",
            "Epoch 396/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4038949.7500 - mae: 4038950.2500 - learning_rate: 14677993.0000\n",
            "Epoch 397/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4361147.5000 - mae: 4361148.5000 - learning_rate: 15848932.0000\n",
            "Epoch 398/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4709070.0000 - mae: 4709070.5000 - learning_rate: 17113284.0000\n",
            "Epoch 399/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5084727.5000 - mae: 5084728.0000 - learning_rate: 18478498.0000\n",
            "Epoch 400/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5490368.5000 - mae: 5490369.0000 - learning_rate: 19952624.0000\n",
            "Epoch 401/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5928359.0000 - mae: 5928359.0000 - learning_rate: 21544346.0000\n",
            "Epoch 402/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6401295.0000 - mae: 6401295.0000 - learning_rate: 23263050.0000\n",
            "Epoch 403/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6911963.0000 - mae: 6911963.5000 - learning_rate: 25118864.0000\n",
            "Epoch 404/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7463362.5000 - mae: 7463362.5000 - learning_rate: 27122726.0000\n",
            "Epoch 405/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8058757.0000 - mae: 8058758.0000 - learning_rate: 29286446.0000\n",
            "Epoch 406/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8701644.0000 - mae: 8701645.0000 - learning_rate: 31622776.0000\n",
            "Epoch 407/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9395822.0000 - mae: 9395822.0000 - learning_rate: 34145488.0000\n",
            "Epoch 408/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10145368.0000 - mae: 10145369.0000 - learning_rate: 36869452.0000\n",
            "Epoch 409/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10954726.0000 - mae: 10954726.0000 - learning_rate: 39810716.0000\n",
            "Epoch 410/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11828630.0000 - mae: 11828631.0000 - learning_rate: 42986624.0000\n",
            "Epoch 411/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12772277.0000 - mae: 12772277.0000 - learning_rate: 46415888.0000\n",
            "Epoch 412/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13791168.0000 - mae: 13791168.0000 - learning_rate: 50118724.0000\n",
            "Epoch 413/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14891381.0000 - mae: 14891382.0000 - learning_rate: 54116952.0000\n",
            "Epoch 414/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16079319.0000 - mae: 16079319.0000 - learning_rate: 58434140.0000\n",
            "Epoch 415/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17362076.0000 - mae: 17362076.0000 - learning_rate: 63095736.0000\n",
            "Epoch 416/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18747110.0000 - mae: 18747110.0000 - learning_rate: 68129208.0000\n",
            "Epoch 417/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20242686.0000 - mae: 20242686.0000 - learning_rate: 73564224.0000\n",
            "Epoch 418/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21857530.0000 - mae: 21857530.0000 - learning_rate: 79432824.0000\n",
            "Epoch 419/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23601246.0000 - mae: 23601246.0000 - learning_rate: 85769592.0000\n",
            "Epoch 420/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25484016.0000 - mae: 25484016.0000 - learning_rate: 92611872.0000\n",
            "Epoch 421/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27517014.0000 - mae: 27517014.0000 - learning_rate: 100000000.0000\n",
            "Epoch 422/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29712186.0000 - mae: 29712186.0000 - learning_rate: 107977520.0000\n",
            "Epoch 423/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32082488.0000 - mae: 32082488.0000 - learning_rate: 116591440.0000\n",
            "Epoch 424/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34641872.0000 - mae: 34641872.0000 - learning_rate: 125892544.0000\n",
            "Epoch 425/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37405412.0000 - mae: 37405412.0000 - learning_rate: 135935632.0000\n",
            "Epoch 426/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40389476.0000 - mae: 40389476.0000 - learning_rate: 146779920.0000\n",
            "Epoch 427/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43611516.0000 - mae: 43611516.0000 - learning_rate: 158489312.0000\n",
            "Epoch 428/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 47090656.0000 - mae: 47090656.0000 - learning_rate: 171132832.0000\n",
            "Epoch 429/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50847308.0000 - mae: 50847308.0000 - learning_rate: 184784976.0000\n",
            "Epoch 430/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54903668.0000 - mae: 54903668.0000 - learning_rate: 199526224.0000\n",
            "Epoch 431/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59283612.0000 - mae: 59283612.0000 - learning_rate: 215443472.0000\n",
            "Epoch 432/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64012972.0000 - mae: 64012972.0000 - learning_rate: 232630512.0000\n",
            "Epoch 433/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69119624.0000 - mae: 69119624.0000 - learning_rate: 251188640.0000\n",
            "Epoch 434/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74633656.0000 - mae: 74633656.0000 - learning_rate: 271227264.0000\n",
            "Epoch 435/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80587544.0000 - mae: 80587544.0000 - learning_rate: 292864448.0000\n",
            "Epoch 436/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87016432.0000 - mae: 87016432.0000 - learning_rate: 316227776.0000\n",
            "Epoch 437/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93958216.0000 - mae: 93958216.0000 - learning_rate: 341454880.0000\n",
            "Epoch 438/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101453680.0000 - mae: 101453680.0000 - learning_rate: 368694496.0000\n",
            "Epoch 439/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109547232.0000 - mae: 109547232.0000 - learning_rate: 398107168.0000\n",
            "Epoch 440/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 118286376.0000 - mae: 118286376.0000 - learning_rate: 429866240.0000\n",
            "Epoch 441/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 127722664.0000 - mae: 127722664.0000 - learning_rate: 464158880.0000\n",
            "Epoch 442/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137911776.0000 - mae: 137911776.0000 - learning_rate: 501187232.0000\n",
            "Epoch 443/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 148913712.0000 - mae: 148913712.0000 - learning_rate: 541169536.0000\n",
            "Epoch 444/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 160793344.0000 - mae: 160793344.0000 - learning_rate: 584341440.0000\n",
            "Epoch 445/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173620608.0000 - mae: 173620608.0000 - learning_rate: 630957376.0000\n",
            "Epoch 446/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187471328.0000 - mae: 187471328.0000 - learning_rate: 681292096.0000\n",
            "Epoch 447/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 202426688.0000 - mae: 202426688.0000 - learning_rate: 735642240.0000\n",
            "Epoch 448/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 218575456.0000 - mae: 218575456.0000 - learning_rate: 794328256.0000\n",
            "Epoch 449/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 236012336.0000 - mae: 236012336.0000 - learning_rate: 857695872.0000\n",
            "Epoch 450/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 254840096.0000 - mae: 254840096.0000 - learning_rate: 926118720.0000\n",
            "Epoch 451/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275170368.0000 - mae: 275170368.0000 - learning_rate: 1000000000.0000\n",
            "Epoch 452/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 297121664.0000 - mae: 297121664.0000 - learning_rate: 1079775104.0000\n",
            "Epoch 453/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 320825024.0000 - mae: 320825024.0000 - learning_rate: 1165914368.0000\n",
            "Epoch 454/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 346418432.0000 - mae: 346418432.0000 - learning_rate: 1258925440.0000\n",
            "Epoch 455/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 374054432.0000 - mae: 374054432.0000 - learning_rate: 1359356416.0000\n",
            "Epoch 456/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 403894464.0000 - mae: 403894464.0000 - learning_rate: 1467799296.0000\n",
            "Epoch 457/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 436115296.0000 - mae: 436115296.0000 - learning_rate: 1584893184.0000\n",
            "Epoch 458/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 470906368.0000 - mae: 470906368.0000 - learning_rate: 1711328256.0000\n",
            "Epoch 459/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 508473248.0000 - mae: 508473248.0000 - learning_rate: 1847849856.0000\n",
            "Epoch 460/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 549036608.0000 - mae: 549036608.0000 - learning_rate: 1995262336.0000\n",
            "Epoch 461/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4364s\u001b[0m 5ms/step - loss: 592836096.0000 - mae: 592836096.0000 - learning_rate: 2154434816.0000\n",
            "Epoch 462/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 640129856.0000 - mae: 640129856.0000 - learning_rate: 2326305024.0000\n",
            "Epoch 463/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 691195904.0000 - mae: 691195904.0000 - learning_rate: 2511886336.0000\n",
            "Epoch 464/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 746336448.0000 - mae: 746336448.0000 - learning_rate: 2712272640.0000\n",
            "Epoch 465/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 805875904.0000 - mae: 805875904.0000 - learning_rate: 2928644608.0000\n",
            "Epoch 466/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 870163840.0000 - mae: 870163840.0000 - learning_rate: 3162277632.0000\n",
            "Epoch 467/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 939582528.0000 - mae: 939582528.0000 - learning_rate: 3414548992.0000\n",
            "Epoch 468/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1014536384.0000 - mae: 1014536384.0000 - learning_rate: 3686945024.0000\n",
            "Epoch 469/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1095472640.0000 - mae: 1095472640.0000 - learning_rate: 3981071616.0000\n",
            "Epoch 470/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1182862592.0000 - mae: 1182862592.0000 - learning_rate: 4298662400.0000\n",
            "Epoch 471/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 177ms/step - loss: 1277228032.0000 - mae: 1277228032.0000 - learning_rate: 4641588736.0000\n",
            "Epoch 472/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1379116032.0000 - mae: 1379116032.0000 - learning_rate: 5011872256.0000\n",
            "Epoch 473/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1489138560.0000 - mae: 1489138560.0000 - learning_rate: 5411695104.0000\n",
            "Epoch 474/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1607931392.0000 - mae: 1607931392.0000 - learning_rate: 5843414016.0000\n",
            "Epoch 475/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1736208512.0000 - mae: 1736208512.0000 - learning_rate: 6309573632.0000\n",
            "Epoch 476/500\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1874710144.0000 - mae: 1874710144.0000 - learning_rate: 6812920832.0000\n",
            "Epoch 477/500\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "window_size = 30\n",
        "\n",
        "train_set = window_dataset(x_train, window_size)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Input(shape=[window_size]),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(10, activation=\"relu\"),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# learning rate scheduler runs over a range of learning rates\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 30))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9)\n",
        "\n",
        "model.compile(loss=keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "\n",
        "# add callback with the learning rate scheduler\n",
        "lr_opt_model = model.fit(train_set, epochs=500, callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdaO305R9GzY",
        "outputId": "dc227ae5-3bc3-4816-928f-e68ce005138c"
      },
      "outputs": [],
      "source": [
        "# forecast based on linear deep learning model\n",
        "dense_forecast = model_forecast(model, series[split_time - window_size:-1], window_size)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "3hFy2T2r9eHW",
        "outputId": "0810a8c2-ee8e-4b51-d81d-611e382d564d"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, dense_forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgkELN-58OIw",
        "outputId": "4c9a3540-8a68-416b-a999-9d687ca15ef6"
      },
      "outputs": [],
      "source": [
        "keras.metrics.mean_absolute_error(x_valid, dense_forecast).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG_sfusvcYnN"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# Source: https://github.com/tensorflow/examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vidayERjaO5q"
      ],
      "include_colab_link": true,
      "name": "ts_04_forecasting_with_ml_student.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
